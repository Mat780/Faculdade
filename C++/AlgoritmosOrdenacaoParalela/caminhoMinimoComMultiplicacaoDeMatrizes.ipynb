{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZBJAtfQ-azC"
      },
      "source": [
        "Alunos: Giovanna Rodrigues Mendes e Matheus Felipe Alves Durães\n",
        "\n",
        "\n",
        "RGAs: 2021.1904.032-7 e 2021.1904.008-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLKbugjZqrx_"
      },
      "source": [
        "# Descrição:\n",
        "#### Fazer duas implementações paralelas em CUDA para resolver o problema de calcular as distâncias de todos os caminhos mínimos em um dado grafo de entrada representado por sua matriz de adjacência, como o visto em sala. Você deve implementar duas soluções paralelas distintas para o problema e  analisar cada uma delas em relação a solução sequencial.\n",
        "\n",
        "#### Você deve comparar o desempenho de as suas soluções com a solução sequencial para grafos com número de vértices entre 512 e 8192  e escrever um relatório com os resultados obtidos. Os códigos fontes juntamente com o relatório em pdf devem ser entregues via AVA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f9Fb1TCqCi0"
      },
      "outputs": [],
      "source": [
        "#Permite a execução dos códigos abaixo\n",
        "!pip install git+https://github.com/lesc-ufv/cad4u.git &> /dev/null\n",
        "!git clone https://github.com/lesc-ufv/cad4u &> /dev/null\n",
        "%load_ext plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaWnjKdwHSfE",
        "outputId": "ddc2e282-5376-41fb-d619-35e57b3204b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Nov 15 20:11:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnVgx-DNM0Vy"
      },
      "source": [
        "## Verificação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEcYxoVxM0sp",
        "outputId": "a6d86429-6fb0-4e15-cdfc-63d4d05eda5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing verify.h\n"
          ]
        }
      ],
      "source": [
        "%%writefile verify.h\n",
        "\n",
        "void checkResults(float *cpu, float *gpu, int size) {\n",
        "  bool pass = true;\n",
        "\n",
        "  for(int i = 0; i < size; ++i) {\n",
        "    if(abs(cpu[i] - gpu[i]) > 0.001) {\n",
        "      printf(\"FAIL: value of position (%d,%d) on cpu %.2f != %.2f on gpu\\n\", i/size, i%size, cpu[i], gpu[i]); pass = false; break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (pass) printf(\"Sucesso! todos os valores foram calculados corretamente!\\n\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFh5OGUu924x"
      },
      "source": [
        "## Medição do tempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7uYogL192Ks",
        "outputId": "128685f1-2b05-455f-f0f4-82044bdfdadc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing time_analisys.h\n"
          ]
        }
      ],
      "source": [
        "%%writefile time_analisys.h\n",
        "\n",
        "float elapsed_time;\n",
        "cudaEvent_t start, stop;                             //# Declara dois eventos\n",
        "\n",
        "void time_start() {\n",
        "  cudaEventCreate(&start);                          //# Irá marcar o inicio da execucao\n",
        "  cudaEventCreate(&stop);                           //# Irá  marcar o final da execucao\n",
        "  cudaEventRecord(start, 0);                        //# Insere na fila\n",
        "}\n",
        "\n",
        "void time_end() {\n",
        "  cudaEventRecord(stop, 0);                          //# Insere na fila\n",
        "  cudaEventSynchronize(stop);                        //# Espera terminar\n",
        "  cudaEventElapsedTime(&elapsed_time, start, stop);  //# Calcula o tempo passado\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tavm5EYyUQJM"
      },
      "source": [
        "## CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCqVp596UQQm",
        "outputId": "a262ed8d-c269-427a-f147-c5a773686cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing cpu.h\n"
          ]
        }
      ],
      "source": [
        "%%writefile cpu.h\n",
        "\n",
        "void cpu_mmatrix(float *hres, float *a, float *b, int n) {\n",
        "    float temp;\n",
        "    float auxA;\n",
        "    float auxB;\n",
        "\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "\n",
        "            if (i == j) {\n",
        "                hres[i*n+j] = 0;\n",
        "\n",
        "            } else if(i > j) {\n",
        "                hres[i*n+j] = 200;\n",
        "\n",
        "            } else {\n",
        "                temp = a[i*n+j];\n",
        "                for (int k = 0; k < n; k++) {\n",
        "                    auxA = a[i*n + k];\n",
        "                    auxB = b[k*n + j];\n",
        "\n",
        "                    if (temp > (auxA + auxB)) {\n",
        "                        temp = auxA + auxB;\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                hres[i*n+j] = temp;\n",
        "            }\n",
        "\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEzOIlgSUtkf"
      },
      "source": [
        "## GPU: Naive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1f26JatUttP",
        "outputId": "7a3bf6bb-3153-49fc-e0de-99584ed820f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing naive.h\n"
          ]
        }
      ],
      "source": [
        "%%writefile naive.h\n",
        "\n",
        "__global__ void matrixMul_naive(float *c, float *a, float *b, int n) {\n",
        "    // Computa a linha e coluna de cada thread\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    // Itera sobre a linha e descendo a coluna\n",
        "    float valorTemp;\n",
        "    float aux;\n",
        "\n",
        "    aux = 200;\n",
        "\n",
        "    if (row == col) {\n",
        "        c[row * n + col] = 0;\n",
        "    } else {\n",
        "        for (int k = 0; k < n; k++) {\n",
        "          valorTemp = a[row * n + k] + b[k * n + col];\n",
        "          if(aux > valorTemp) {\n",
        "              aux = valorTemp;\n",
        "          }\n",
        "        }\n",
        "\n",
        "        c[row * n + col] = aux;\n",
        "    }\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1xxvrMeFTbo"
      },
      "source": [
        "## GPU: DNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkpAcVhpDAfD",
        "outputId": "871b8fc5-402c-4f99-87fa-32abd595a571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting matriz_mul.h\n"
          ]
        }
      ],
      "source": [
        "%%writefile matriz_DNS.h\n",
        "\n",
        "__global__ void matrizMul_DNS(float *ans, float *x, float *y, int n) {\n",
        "\n",
        "    // Calcula o ID unico da thread no bloco\n",
        "    int t = (blockDim.x * blockDim.y) * threadIdx.z + (threadIdx.y * blockDim.x) + threadIdx.x;\n",
        "\n",
        "    // Calcula o ID unico do bloco na grid\n",
        "    int b = (gridDim.x * gridDim.y) * blockIdx.z + (blockIdx.y * gridDim.x) + (blockIdx.x);\n",
        "\n",
        "    // Tamanho do bloco (Pode ser redundante)\n",
        "    int tamanhoDoBloco = blockDim.x*blockDim.y*blockDim.z;\n",
        "\n",
        "    // Tamanho da Grid (Pod ser redundante)\n",
        "    int tamanhoDaGrid = gridDim.x*gridDim.y*gridDim.z;\n",
        "\n",
        "    float aux;\n",
        "    float valorTemp;\n",
        "\n",
        "    for (int i = b; i < n ; i+= tamanhoDaGrid) {\n",
        "        for (int j = t; j < n ;j += tamanhoDoBloco) {\n",
        "            if (i == j) {\n",
        "                aux = 0;\n",
        "\n",
        "            } else {\n",
        "                aux = 200;\n",
        "\n",
        "                for (int k = 0; k < n ;k++) {\n",
        "                    valorTemp = x[i * n + k] + y[k * n + j];\n",
        "                    if (aux > valorTemp) {\n",
        "                        aux = valorTemp;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            ans[i * n + j] = aux;\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVIFOY4pu1Ni"
      },
      "source": [
        "## Caminho Míınimo: Multiplicação de matrizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9zVKXhYCwrK",
        "outputId": "cca3eec2-69c1-46c9-aec2-f1b722d6ad78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==26125== NVPROF is profiling process 26125, command: /content/code.out\n",
            "Dispositivo 0: Tesla T4 \n",
            "Ocupação tamanho da Matriz 268435456\n",
            "Blocos: 256\n",
            "Threads/blocos: 32\n",
            "Threads(total): 8192\n",
            "\n",
            "Tempo GPU naive:     18304.20 ms\n",
            "Tempo GPU caminhoMin: 9518.98 ms\n",
            "==26125== Profiling application: /content/code.out\n",
            "==26125== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.13%  18.3037s         1  18.3037s  18.3037s  18.3037s  matrixMul_naive(float*, float*, float*, int)\n",
            "                   33.87%  9.51868s         1  9.51868s  9.51868s  9.51868s  matrizMul_caminhoMin(float*, float*, float*, int)\n",
            "                    0.61%  170.24ms         3  56.746ms  55.621ms  57.722ms  [CUDA memcpy HtoD]\n",
            "                    0.40%  112.19ms         2  56.095ms  55.826ms  56.364ms  [CUDA memcpy DtoH]\n",
            "      API calls:   98.50%  27.8224s         2  13.9112s  9.51870s  18.3037s  cudaDeviceSynchronize\n",
            "                    1.01%  283.89ms         5  56.778ms  55.854ms  57.907ms  cudaMemcpy\n",
            "                    0.48%  134.52ms         3  44.842ms  318.73us  133.89ms  cudaMalloc\n",
            "                    0.01%  3.7864ms         3  1.2621ms  407.75us  2.1135ms  cudaFree\n",
            "                    0.00%  114.18us       101  1.1300us     144ns  47.704us  cuDeviceGetAttribute\n",
            "                    0.00%  84.383us         1  84.383us  84.383us  84.383us  cudaGetDeviceProperties\n",
            "                    0.00%  64.158us         2  32.079us  31.442us  32.716us  cudaLaunchKernel\n",
            "                    0.00%  51.485us         4  12.871us  4.5980us  22.390us  cudaEventRecord\n",
            "                    0.00%  27.434us         4  6.8580us     996ns  18.094us  cudaEventCreate\n",
            "                    0.00%  23.347us         1  23.347us  23.347us  23.347us  cuDeviceGetName\n",
            "                    0.00%  10.592us         2  5.2960us  3.9360us  6.6560us  cudaEventSynchronize\n",
            "                    0.00%  7.0540us         2  3.5270us  2.9280us  4.1260us  cudaEventElapsedTime\n",
            "                    0.00%  6.4540us         1  6.4540us  6.4540us  6.4540us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.2480us         1  3.2480us  3.2480us  3.2480us  cudaSetDevice\n",
            "                    0.00%  1.8020us         3     600ns     249ns  1.2640us  cuDeviceGetCount\n",
            "                    0.00%  1.0710us         1  1.0710us  1.0710us  1.0710us  cudaGetLastError\n",
            "                    0.00%  1.0280us         2     514ns     277ns     751ns  cuDeviceGet\n",
            "                    0.00%     514ns         1     514ns     514ns     514ns  cuDeviceTotalMem\n",
            "                    0.00%     457ns         1     457ns     457ns     457ns  cuModuleGetLoadingMode\n",
            "                    0.00%     241ns         1     241ns     241ns     241ns  cuDeviceGetUuid\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%nvprof\n",
        "\n",
        "#define N 8192  // Matrix NxN\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <math.h>\n",
        "#include \"/content/verify.h\"\n",
        "#include \"/content/time_analisys.h\"\n",
        "#include \"/content/cpu.h\"\n",
        "#include \"/content/naive.h\"\n",
        "#include \"/content/matriz_DNS.h\"\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "    int deviceId, numberOfSMs;\n",
        "    clock_t comeco, fim;\n",
        "\n",
        "    cudaDeviceProp deviceProp;\n",
        "    cudaGetDeviceProperties(&deviceProp, 0);\n",
        "    printf(\"Dispositivo %d: %s \\n\", 0, deviceProp.name);\n",
        "    cudaSetDevice(0);\n",
        "\n",
        "    size_t bytes = N * N * sizeof(float);\n",
        "\n",
        "    printf(\"Ocupação tamanho da Matriz %d\\n\", bytes);\n",
        "\n",
        "    float *h_a, *h_cpu, *h_naive, *h_DNS;\n",
        "    float *d_a, *d_naive, *d_DNS;\n",
        "    float time_cpu, time_gpu_naive, time_gpu_DNS;\n",
        "\n",
        "    h_a = (float*) malloc(bytes);\n",
        "    h_cpu = (float*) malloc(bytes);\n",
        "    h_naive = (float*) malloc(bytes);\n",
        "    h_DNS = (float*) malloc(bytes);\n",
        "\n",
        "    srand(1);\n",
        "    for (int k = 0; k < N; k++) {\n",
        "        int aux;\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            if (i > k) {\n",
        "                aux = rand() % 20;\n",
        "\n",
        "                aux = aux >= 15 ? 200 : aux;\n",
        "\n",
        "                h_a[i + (k*N)] = aux;\n",
        "\n",
        "            } else if (i < k) {\n",
        "                h_a[i + (k*N)] = 200.0;\n",
        "\n",
        "            } else {\n",
        "                h_a[i + (k*N)] = 0;\n",
        "            }\n",
        "\n",
        "        }\n",
        "    }\n",
        "\n",
        "    /* # Print de debug para saber a matriz inicial\n",
        "    printf(\"------------------------------------------------\\n\");\n",
        "\n",
        "    for (int i = 0; i < (N*N); i++) {\n",
        "        printf(\"%3.0f \", h_a[i]);\n",
        "        if ((i % N) == (N-1)) printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"------------------------------------------------\\n\");\n",
        "    */\n",
        "\n",
        "    memset(h_cpu, 0, bytes);\n",
        "    memset(h_naive, 0, bytes);\n",
        "    memset(h_DNS, 0, bytes);\n",
        "\n",
        "    cudaMalloc(&d_a, bytes);\n",
        "    cudaMalloc(&d_naive, bytes);\n",
        "    cudaMalloc(&d_DNS, bytes);\n",
        "\n",
        "    //* Copia os dados para o dispositivo\n",
        "    if (cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice) != 0) {\n",
        "        printf(\"Erro ao copiar dados para d_a\\n\");\n",
        "    }\n",
        "    if (cudaMemcpy(d_naive, h_naive, bytes, cudaMemcpyHostToDevice) != 0){\n",
        "        printf(\"Erro ao copiar dados para d_naive\\n\");\n",
        "    }\n",
        "    if (cudaMemcpy(d_DNS, h_DNS, bytes, cudaMemcpyHostToDevice) != 0){\n",
        "        printf(\"Erro ao copiar dados para d_DNS\\n\");\n",
        "    }\n",
        "\n",
        "    //* Execucao na CPU\n",
        "    /* # Execução CPU\n",
        "    comeco = clock();\n",
        "    cpu_mmatrix(h_cpu, h_a, h_a, N);\n",
        "    fim = clock() - comeco;\n",
        "    time_cpu = 1000*((float)fim) / CLOCKS_PER_SEC;\n",
        "    */\n",
        "\n",
        "    // Threads por dimensão CTA\n",
        "    int THREADS = 32; //# Threads aqui!\n",
        "\n",
        "    // Blocos por dimensão de grid (assume que THREADS vai dividir N igualmente)\n",
        "    int BLOCKS = N / THREADS;\n",
        "\n",
        "    // Use dim3 structs for block  and grid dimensions\n",
        "    dim3 threads(THREADS, THREADS);\n",
        "    dim3 blocks(BLOCKS, BLOCKS);\n",
        "\n",
        "    printf(\"Blocos: %d\\nThreads/blocos: %d\\nThreads(total): %d\\n\\n\", BLOCKS, THREADS, THREADS*BLOCKS);\n",
        "\n",
        "    //printf(\"Tempo de CPU: %15.2lf ms\\n\", time_cpu);\n",
        "\n",
        "    //* Chamada do kernel naive\n",
        "    time_start();\n",
        "    matrixMul_naive<<<blocks, threads>>>(d_naive, d_a, d_a, N);\n",
        "    cudaDeviceSynchronize();\n",
        "    time_end();\n",
        "\n",
        "    if (cudaMemcpy(h_naive, d_naive, bytes, cudaMemcpyDeviceToHost) != 0) {\n",
        "        printf(\"Erro ao copiar resultados de d_naive\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"Tempo GPU naive: %12.2lf ms\\n\", elapsed_time);\n",
        "\n",
        "    //* Chamada do kernel DNS\n",
        "    time_start();\n",
        "    matrizMul_DNS<<<blocks, threads>>>(d_DNS, d_a, d_a, N);\n",
        "    cudaDeviceSynchronize();\n",
        "    time_end();\n",
        "\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"CUDA error: %s\\n\" ,cudaGetErrorString(err));\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "    if (cudaMemcpy(h_DNS, d_DNS, bytes, cudaMemcpyDeviceToHost) != 0) {\n",
        "        printf(\"Erro ao copiar resultados de d_DNS\\n\");\n",
        "        exit(1);\n",
        "    }\n",
        "\n",
        "    printf(\"Tempo GPU DNS: %7.2lf ms\\n\", elapsed_time);\n",
        "\n",
        "    /*\n",
        "    printf(\"------------------------------------------\\nCPU: \\n\");\n",
        "\n",
        "    for (int i = 0; i < (N*N); i++) {\n",
        "        printf(\"%3.0f \", h_cpu[i]);\n",
        "        if ((i % N) == (N-1)) printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"------------------------------------------\\nGPU Naive: \\n\");\n",
        "\n",
        "    for (int i = 0; i < (N*N); i++) {\n",
        "        printf(\"%3.0f \", h_naive[i]);\n",
        "        if ((i % N) == (N-1)) printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"------------------------------------------\\nGPU DNS: \\n\");\n",
        "\n",
        "    for (int i = 0; i < (N*N); i++) {\n",
        "        printf(\"%3.0f \", h_DNS[i]);\n",
        "        if ((i % N) == (N-1)) printf(\"\\n\");\n",
        "    }\n",
        "    */\n",
        "\n",
        "    //# Checkresults desligados para testar maiores valores de N sem a CPU\n",
        "    //checkResults(h_cpu, h_naive, N*N);\n",
        "    //checkResults(h_cpu, h_DNS, N*N);\n",
        "\n",
        "    free(h_cpu); free(h_naive); free(h_a); free(h_DNS);\n",
        "    cudaFree(d_naive); cudaFree(d_a); cudaFree(d_DNS);\n",
        "    return 0;\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
